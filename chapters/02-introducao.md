### Introdução

Durante a pandemia da Covid-19, gráficos representando o aumento do número de casos ao longo do tempo, por meio de curvas de distribuição, ganharam as notícias e o debate público. A visualização de dados se tornou &#39;mainstream&#39;, com o apelo de salvar vidas. Gráficos são debatidos por especialistas e leigos nas redes sociais, e a interpretação dos dados se revela uma habilidade crucial para os governos conseguirem controlar a pandemia, assim como a qualidade das estatísticas oficiais.

A escala do coronavírus e suas implicações não têm precedentes, mas a visualização de dados sobre mortalidade tem um histórico que se confunde com os próprios primórdios da prática de entender e comunicar dados por meio de gráficos. Dois dos trabalhos mais icônicos da história da comunicação baseada em dados estão intimamente ligados à área de saúde e também buscavam, em última instância, salvar vidas.

Um deles é o gráfico de Florence Nightingale sobre as mortes durante a Guerra da Crimeia. Hoje considerada mãe da enfermagem moderna, ela esteve à frente dos cuidados aos soldados ingleses durante as batalhas e também compilou estatísticas sobre a causa de mortalidade dos combatentes. Por meio de um gráfico elaborado em 1858, hoje considerado um clássico, ela mostrou que as mortes em combate eram minoritárias, se comparadas àquelas causadas por doenças preveníveis, como aquelas provocadas por má condições de higienes no hospital.

Esta forma de representar os dados é conhecida como diagrama de área polar (ou _coxcomb,_ em inglês) e é utilizada até os dias atuais. Na imagem, é possível ver como a proporção de mortes por causas evitáveis era constantemente maior ao longo dos meses.

![Florence Nightingale]

Além de diversas guerras e batalhas, o século XIX teve também seis epidemias de cólera, que mataram milhões de pessoas em diversos continentes. E foi na mesma Inglaterra dos anos 1850 que o médico John Snow decidiu levantar dados &quot;geolocalizados&quot; sobre a doença, mapeando os casos e óbitos. Assim, ele elaborou o chamado &quot;mapa da cólera&quot;, que representou os casos da doença como pontos no mapa. A partir da análise dos padrões espaciais e uma intensiva investigação por meio de entrevistas, descobriu uma bomba d&#39;água como vetor de disseminação da cólera no bairro de Soho. À época, Londres experimentava uma expansão urbana somada ao crescimento populacional em condições precárias de moradia e saneamento.

Com isso, John Snow reforçou sua teoria de que a água poderia ser um dos vetores de contaminação, na contramão da chamada teoria miasmática, que era predominante à época e supunha a transmissão por meio do ar. A história mostrou que ele estava certo. Hoje, Snow é considerado um dos nomes fundadores da epidemiologia.

! [IMG 2- mapa Snow / mapa coronavírus]

Retomamos os exemplos de Snow e Nightingale por uma dupla razão: primeiro, para desfazer um aura de novidade (&quot;hype&quot;) em torno do tema. Lidar com dados não é exatamente algo recente na experiência humana, tampouco teve início com os computadores digitais. Na verdade, no fundo, a utilização de dados é praticamente indissociável da experiência humana tal como a conhecemos.

Os exemplos podem ser diversos. Podem ser até pré-históricos, como quando as contagens eram feitas em artefatos rudimentares, como o osso de Ishango, objeto datado do paleolítico que foi encontrado na África Central e hoje é considerado uma das primeiras formas de armazenamento de dados. Ou ainda o ábaco, inventado durante a Antiguidade no Oriente Médio para facilitar cálculos, e a máquina de Anticítera, um computador analógico para fazer previsões sobre eclipses e as posições dos astros, que foi inventado pelos gregos. Sem dúvida, as tecnologias digitais de armazenamento de dados atuais abrem horizontes novos e bastante amplos, mas é importante lembrar que lidamos com a quantificação da realidade cotidianamente e desde sempre.

Quanto tempo irei levar para chegar ao meu compromisso? Quantas xícaras de farinha preciso colocar no bolo? Qual a diferença de preço entre este e aquele produto? Quantos quilos eu ganhei depois do Natal? Como podemos dividir uma conta igualmente entre as pessoas? São perguntas banais, mas ilustram como sempre lidamos com dados no cotidiano em algum nível, mesmo sem refletir muito sobre isso.

Assim, por um lado, isso deixa claro que qualquer pessoa pode vir a trabalhar com dados. Esta prática não é e não precisa ser restrita a especialistas ou pessoas ligadas às chamadas &quot;áreas de exatas&quot; (matemática, estatística, desenvolvedores de software, etc). Por outro, a segunda razão para retomarmos o exemplo de Nightingale e Snow é para ressaltar um cuidado importante relacionado à comunicação baseada em dados: qualquer pessoa poder trabalhar com dados, mas isso não significa que eles devam ser tratados de qualquer forma.

Há alguns ensinamentos que podemos extrair das experiências do século XIX que foram mencionadas, particularmente no que diz respeito ao método científico. Esse tipo de procedimento nos ajuda a ter uma leitura crítica dos dados para utilizá-los como uma ferramenta de conhecimento - e não como artifício retórico para reforçar crenças e opiniões pré-existentes.

A ciência pressupõe o acúmulo de conhecimento. Cada cientista estuda uma pequena parte de um tema, ocupando-se de um problema muito específico. A partir de muita leitura - a revisão bibliográfica serve para conhecer o estado da arte de determinada área, ou seja, aquilo que outros pesquisadores já descreveram - e observação da realidade, o cientista está apto a formular e testar sua hipótese.

A descrição detalhada do método é fundamental para que a comunidade científica seja capaz de validar os resultados de uma pesquisa e eventualmente replicá-la integral ou parcialmente. Na academia, os artigos são submetidos à revisão cega por pares - em outras palavras, significa dizer que outros cientistas leram e avaliaram a consistência teórica e metodológica de um trabalho. Em projetos mais complexos e longos, como teses e dissertações, o mesmo ocorre a partir de uma banca avaliadora.

No imaginário social, o estereótipo do cientista geralmente é formado por uma pessoa de jaleco branco que trabalha em um laboratório conduzindo experimentos químicos. A realidade, no entanto, sugere uma fotografia mais ampla: cientistas também usam roupas normais e trabalham apenas no computador em salas convencionais nas universidades ou mesmo de casa. Assim também se faz ciência, e com métodos igualmente rigorosos.

Neste guia, não iremos abordar a área de ciência de dados em si, ainda que muitos dos conceitos e técnicas que iremos abordar aqui sejam emprestados desta área. Nosso enfoque aqui será a comunicação baseada em dados, especialmente na área do jornalismo. Da ciência, iremos tomar emprestado principalmente o método para que possamos conduzir trabalhos mais sólidos e robustos.

Embora o jornalismo não seja uma ciência propriamente dita, visto que trabalha com a ideia de pauta e de um conhecimento menos formal e sistemático, pode aproximar-se do rigor metodológico à medida que abre suas fontes, seus dados e o passo a passo empreendido para obter um resultado. É neste contexto que a ideia de replicabilidade entra em cena e pode ser apropriada tanto pelos leitores quanto por outros jornalistas. Ao longo deste guia, veremos como podemos nos inspirar no método científico para produzir conteúdos baseados em dados com mais rigor e qualidade.

Em tempos de debates acalorados na internet, é comum que as pessoas busquem os dados apenas quando querem legitimar seus argumentos. Há quem fale até em &quot;torturar os dados&quot; para que confessem o que o analista deseja. Aqui, porém, trabalharemos a perspectiva oposta.

É importante que você seja cético em relação aos seus dados. A respeito deste tema, vale conferir o artigo _&quot;_[_Sou uma cientista de dados cética quanto aos dados_](https://escoladedados.org/2019/08/sou-uma-cientista-de-dados-cetica-quanto-aos-dados/)_&quot;_ de Andrea Jones-Rooy, professora de Ciência de Dados na New York University. Ao trabalhar com investigações baseadas em dados, seu objetivo não é encontrar números que confirmem sua hipótese ou aquilo que você (acha que) sabe. Ao contrário, você deve olhar os dados sob uma perspectiva crítica, ao invés de adotá-los como uma representação objetiva da realidade.

Em especial se você não é um especialista no tema que está abordando, provavelmente, sua autocrítica talvez não seja capaz de avaliar todas as possíveis nuances dos dados ou erros de uma análise. Por isso, para fazer sentido de tabelas ou dados, é importante também conhecer como eles foram produzidos e, se possível, entrevistar ou conversar com diferentes pessoas que entendam mais que você sobre o tema em questão, seja o novo coronavírus, a performance de um clube em um campeonato esportivo ou mesmo dados sobre o mercado de trabalho.

No fim das contas, como na ciência, a transparência também é fundamental na comunicação. Então, sempre que possível, disponibilize os dados utilizados ou a referência de onde eles se encontram. Dessa forma, as pessoas poderão refazer o passo a passo da sua análise - remetendo ao princípio da replicabilidade científica - se assim desejar. Aprender a programar também ajuda, pois assim você consegue documentar e compartilhar todas as etapas de processamento, análise e até visualização dos dados. Além de tornar seu trabalho mais confiável, a publicação de scripts e programas com código-aberto permite que outras pessoas possam revisar o trabalho feito, identificar possíveis erros (sim, isso é ótimo!), entender como ele foi feito e aprender com a sua experiência.

Visto que o jornalismo envolve a produção de conhecimento a partir da observação da realidade, semelhante ao que ocorre na ciência, desenvolveram-se diversas teorias e práticas para aproximar estes dois campos ao longo do século XX. O primeiro grande esforço sistemático neste sentido foi feito por Philip Meyer, criador do chamado &quot;jornalismo de precisão&quot;.

Meyer se destacou por aplicar métodos das ciências sociais no jornalismo. No ano de 1967, em meio à luta do movimento negro por direitos civis nos EUA, durante a cobertura de protestos em Detroit, ele decidiu aplicar técnicas de pesquisas amostrais (survey) para entender a opinião pública e causas das manifestações, a partir da perspectiva da população local. Algo com intuito parecido ao que um repórter ou uma organização faz ao coletar depoimentos de pessoas sobre determinado tema, porém, com o objetivo de refletir a opinião coletiva de um grupo social, ao invés de opiniões individuais.

Ainda nos anos 1960, Philip Meyer e sua equipe criaram uma amostra das residências em regiões de manifestações, entrevistaram 437 pessoas e utilizaram um computador para testar hipóteses e processar os resultados, [culminando nesta apresentação](https://s3.amazonaws.com/s3.documentcloud.org/documents/2070181/detroit1967.pdf). Na contramão de discursos que circulavam à època, inclusive na própria imprensa, a pesquisa mostrou que que os manifestantes não eram pessoas de baixa educação e que apoiavam atos violentos. Formação ou renda não eram fatores distintivos entre aqueles que participavam dos atos, mas o desemprego, sim. Além disso, os participantes da pesquisa apontaram possíveis causas para os atos violentos, que até então não recebiam a devida atenção, como violência policial ou condições de habitação precárias.

Ao defender uma guinada do jornalismo em direção à ciência, Meyer acreditava na ampliação do papel do repórter e na menor dependência de fontes oficiais. Dessa forma, como no caso de Detroit, acreditava que os próprios jornalistas deveriam fazer a coleta de dados para responder às questões levantadas no dia a dia da profissão. A cobertura do colapso social do final da década de 1960 rendeu a Meyer e sua equipe o primeiro lugar na categoria de jornalismo local no Prêmio Pulitzer.

Posteriormente, o jornalismo de precisão inspirou práticas como a Reportagem Assistida por Computador (RAC) - ou Computer-Assisted Reporting (CAR), em inglês -, desenvolvida especialmente nos anos de 1980 nos Estados Unidos. Com a disseminação do uso de computadores no ambiente de trabalho dos jornalistas, surgiram novidades nos procedimentos de apuração. Nos primórdios da RAC, o uso de softwares simples, como editores de planilha, para resolver problemas e obter informações, já era motivo para comemorar.

No final dos anos de 1990, Meyer sugeriu que se pensasse em outro nome para substituir a RAC. Ele admitiu que o termo ficou datado por apresentar a palavra &quot;computador&quot; em sua composição, destacando uma ferramenta utilizada para a apuração jornalística. Tampouco o &quot;jornalismo de precisão&quot; obteve sucesso como conceito amplamente difundido.

Atualmente, porém, parte das premissas do jornalismo de precisão e da RAC são levadas a cabo por práticas do chamado jornalismo de dados. Dados são abundantes e habilidades básicas de análise são exigidas atualmente em diversos campos da comunicação. O jornalismo de dados - também conhecido como jornalismo guiado por dados (data-driven journalism) - pode envolver todas as etapas do trabalho, desde a coleta até a visualização das informações.

Quando publicou o artigo[_&quot;_](http://www.holovaty.com/writing/fundamental-change/)[_A fundamental way newspaper sites need to change_](http://www.holovaty.com/writing/fundamental-change/)&quot;, em 2006, o programador Adrian Holovaty defendia que os jornalistas se preocupassem com o uso de informações estruturadas. Isto é, que os repórteres não coletassem apenas informações para uma pauta específica, mas que refletissem sobre sua organização sistemática, facilitando a recuperação e o reuso do que já foi apurado. Em vez de somente divulgar informações sobre um incêndio específico, por exemplo, os jornalistas deveriam coletar aspectos que todos os incêndios têm em comum (localização, número de vítimas, tempo de atendimento, prejuízo etc.) para, com a formatação de um banco de dados, identificar padrões e obter informações mais abrangentes e contextuais.

Embora estes recursos ainda sejam utilizados hoje e as reflexões de Holovaty sigam atuais, planilhas e consultas online em portais da transparência remetem à primeira metade dos anos 2000, em que havia menos conhecimento técnico e disponibilidade de matéria-prima aberta e acessível aos jornalistas de dados. O professor Paul Bradshaw classifica a segunda década (2010-2020) do jornalismo de dados como uma &quot;[segunda onda](https://medium.com/@paulbradshaw/the-next-wave-of-data-journalism-7e2e10087bb3)&quot;, permeada por um movimento global pela abertura de dados, acesso às APIs e a inserção de algoritmos neste cenário, incluindo inteligência artificial e a internet das coisas.

Bradshaw defende que os jornalistas de dados abram seus códigos e submetam suas fontes e métodos ao escrutínio público, pois assim os cidadãos ganham conhecimento sobre sua própria realidade. Ao obter informações sobre a sociedade onde vivem, as pessoas passam a ter condições de cobrar o poder público com mais veemência sobre suas demandas, por exemplo. Outro ponto destacado é o potencial que o jornalismo de dados tem em dar voz às minorias ao divulgar informações quantificáveis e que tendem a circular pouco. Assim, é possível perceber que não só a tecnologia importa ao jornalismo de dados - as demais premissas da prática jornalística, relacionadas à produção de informação qualificada e à fiscalização do poder público, são mantidas e intensificadas.

Como mencionamos, uma das características do jornalismo de dados desta &quot;segunda onda&quot; é o uso de algoritmos de inteligência artificial. Em um texto sobre o assunto, o artigo _&quot;_[_How you&#39;re feeling when machine learning might help_](https://qz.ai/how-youre-feeling-when-machine-learning-might-help/)_&quot;_, Jeremy B. Merril elenca algumas características úteis deste tipo de tecnologia para jornalistas. Ele desta a possibilidade dessas técnicas processarem um volume enorme de documentos, que seriam impossíveis de ser lido por humanos, em busca de padrões.

Ele cita o exemplo do Atlanta Journal-Constitution, que usou o aprendizado de máquina para identificar relatórios de sanções contra médicos relacionados a abuso sexual. A equipe de dados do jornal desenvolveu cerca de 50 raspadores para diferentes sites, que permitiram a coleta de mais de [100 mil documentos de agências reguladoras](http://doctors.ajc.com/about_this_investigation/). Como era impossível ler e categorizar manualmente todos os relatórios, eles selecionaram uma amostra relevante e fizeram esta categorização manual, escolhendo palavras-chave que identificavam relatos de abuso sexual dos demais casos.

Então, treinaram uma solução de aprendizado de máquina que baseado nessas palavras assinalava quais relatórios eram provavelmente mais interessantes para a investigação. Claro que a solução não nasce pronta. Foi necessário encontrar as melhores palavras-chaves por meio de tentativa e erro, observando os resultados das escolhas utilizadas. Porém, é um exemplo interessante do potencial da inteligência artificial para lidar com uma documentação muito vasta.

Neste sentido, uma solução simples, de código-aberto e interessante tanto para jornalistas quanto para organizações é o [DocumentCloud](https://www.documentcloud.org/), uma ferramenta que consegue &quot;ler&quot; dados em documentos, tabelas e PDFs para então realizar a extração de entidades automaticamente, identificando menções a pessoas, locais ou datas em cada documento.

Outros casos de uso mencionados por Merril incluem a chamada computação visual e a identificação de dados que possuam um certo padrão. As aplicações são várias e vale ler o artigo completo, caso você tenha interesse no tema. Por ora, o importante é saber que a inteligência artificial e o aprendizado de máquina permitem converter uma quantidade incontrolável de informações em algo gerenciável. Porém, ainda assim, como qualquer fonte, o computador pode estar errado, confuso ou não ter entendido suas instruções.

## Nada dado nos dados

O que são dados, afinal? Essa é uma pergunta importante. Existem várias definições possíveis. De modo geral, podemos dizer que a produção de dados é baseada na quantificação ou estruturação do mundo. Mencionamos &quot;produção&quot; não por acaso: dados não dão em árvore, ou seja, são entidades que não existem por si só na natureza.

Dados são construções humanas para tentar abstrair (ou tentar &quot;representar&quot;, com muitas aspas) fenômenos complexos da realidade através de elementos quantificáveis, que por sua vez são passíveis de serem analisados. Um dado sempre implica um determinado escopo, um recorte. Eles são construídos baseados em conceitos e, por vezes, em preconceitos. Os dados não são neutros ou puramente objetivos, eles traduzem visões de mundo.

Em suma, não há nada dado nos dados. Dados são sempre construídos e, por serem construções humanas, assim como nós, estão sujeitos a erros de diferentes tipos. Devemos ter sempre ter isso em mente ao analisá-los.

Os dados podem ser encarados como uma fonte como outra qualquer, embora sejam &quot;entrevistados&quot; de forma diferente. Assim como você não considera automaticamente verdade tudo que uma pessoa diz, você também não deve &quot;comprar pelo valor de face&quot; tudo que os dados apontam.

Imagine esta cena: um repórter recebe a ligação de uma fonte, ou seja, uma pessoa que tem coisas interessantes para contar e que quer que essas coisas sejam publicadas no jornal. Essa tal fonte pode ser um assessor de imprensa, um político, um dissidente com documentos para vazar, um juiz, um promotor, um pesquisador. De todo modo, ela tem interesses pessoais – que podem ser completamente nobres – embutidos nas informações que divulga.

Do mesmo modo, também pode ser fontes bancos de dados, estudos científicos, pesquisas das mais diversas naturezas, documentos públicos, legislações, sites de redes sociais e outros. A diferença é que essas fontes não podem ser acessadas a partir de pergunta e resposta diretas; elas pressupõem outro tipo de consulta, que não alteram sua natureza documental.

Ainda que apresentem caráter oficial, bancos de dados e outras fontes documentais podem servir para a obtenção de pautas exclusivas, visto que não estão estruturadas em formato de release nem são disparadas para a imprensa de modo geral. Exigem, sim, o esforço do repórter na elaboração de uma hipótese ou pergunta que dará início à investigação e, depois, na coleta e no tratamento das informações. Para valorizar ainda mais o caráter exclusivo, algumas redações optam por construir os seus próprios bancos de dados, realizando análises inéditas.

Em sua tese de doutorado, o pesquisador Marcelo Träsel entrevistou diversos jornalistas brasileiros que usam dados em suas tarefas cotidianas na redação.

Por meio dos procedimentos de apuração - que costumam ter etapas mais bem delineadas na execução do jornalismo de dados em comparação às práticas tradicionais -, esses profissionais acreditam estar mais próximos da noção de objetividade no método jornalístico, cuja concretização leva em conta a correspondência do relato jornalístico à realidade dos fatos.

O poder de mergulhar em números permite ao repórter contestar as narrativas mais óbvias oferecidas por fontes institucionais.Além disso, na contramão de simplesmente reproduzir declarações ou noticiar informações de bastidores, jornalistas de dados se utilizam de recursos (números e planilhas) que podem ser facilmente disponibilizados e verificados por terceiros (leitores e colegas da imprensa, por exemplo).

Como já dissemos, os bancos de dados entram no cardápio de fontes, e devem ser consideradas como tal - vistas de forma crítica, portanto. Para caracterizar seu trabalho como jornalista de dados, Kátia Brembatti, do jornal Gazeta do Povo, fez uma alusão a &quot;entrevistar planilhas&quot; enquanto participante da pesquisa, expressão que se tornaria o título da tese de Träsel.

Brembatti assinou uma série de reportagens conhecida como &quot;Diários Secretos&quot;, que expôs um esquema de nomeação de funcionários fantasmas na Assembleia Legislativa do Paraná. Seu trabalho consistiu em organizar uma série de informações em arquivos de Excel para descobrir fatos de interesse público.

O esquema envolvia esconder alguns dos Diários Oficiais que continham referências às nomeações irregulares - e o trabalho de reportagem foi reunir documentos que expunham toda a trama. À medida que os dados eram sistematizados, a equipe envolvida na investigação conseguia respostas para suas perguntas, quase como se estivesse ganhando a confiança de um informante particularmente ruim de lidar.

Pensar nos dados como um entrevistado faz bastante sentido – principalmente porque desmistifica a ideia de que um dado é um fato encerrado, definitivo. Como qualquer entrevistado, um banco de dados tem sua história, seus vieses e um bom motivo para apresentar as informações da forma que está apresentando.

Por esses e outros motivos, há críticas quanto ao termo &quot;dados brutos&quot;, já que em sua origem essas informações foram coletadas, selecionadas e disponibilizadas por pessoas. Dados nunca são &quot;brutos&quot;, são sempre moldados por uma forma de abstrair uma realidade complexa em termos quantificáveis.

Por isso, é comum que, ao longo de um trabalho, os dados se tornem cada vez menos autoevidentes. Ao aprofundar suficientemente um determinado tema, os dados trazem consigo, muitas vezes, um emaranhado de questões sobre a forma como foram criados. Assim, vale consultar com atenção as notas metodológicas e os dicionários de variáveis que geralmente acompanham as bases de dados. A leitura da documentação é essencial para compreender o escopo e as limitações de determinado conjunto de informações, evitando problemas nas análises.

## Trabalhando com dados

Saber lidar com dados abre caminhos para quem deseja ter mais autonomia e ser menos dependente de informes oficiais ou trabalhos de terceiros para analisar informações e obter suas próprias conclusões. O percurso de aprendizagem não é óbvio e muitas vezes não está disponível na grade curricular das universidades. Dessa forma, os profissionais costumam buscar por conta própria conhecimentos relacionados ao &quot;letramento em dados&quot; (data literacy) e outros tópicos.

Se este é o seu primeiro contato com o universo dos dados, não se preocupe. Muitas pessoas não se interessam em trilhar esses caminhos por se sentirem inaptas a trabalhar com números ou programação. No entanto, isso pode ser enganoso. A facilidade em trabalhar com estatísticas ou escrever códigos no computador não precisa ser um ponto de partida, mas pode ser algo que você adquire durante o aprendizado. Se você sente essa dificuldade, tudo que você precisa é ter cuidado na hora de comunicar os resultados das suas análises e pedir ajuda de pessoas com mais conhecimentos, quando necessário.

Além disso, atualmente, é possível realizar análises e visualizações bastante interessantes, fazendo tudo por meio de interfaces gráficas. De fato, em muitos trabalhos, especialmente aqueles que requerem pouca customização, importa menos a solução tecnológica utilizada e mais o conteúdo e o impacto da ação em si.

Existem diversos serviços e programas gratuitos que permitem a criação de trabalhos sofisticados sem a necessidade de escrever uma linha de código, como o [Workbench](https://workbenchdata.com/), [Tableau](https://www.tableau.com/pt-br) ou mesmo o [Orange](https://orange.biolab.si/), que trabalha até com algoritmos de inteligência artificial. Neste guia, iremos citar alguns deles e dar algumas dicas de ferramentas, mas nosso foco será te passar uma base que te permitirá lidar com diferentes plataformas e tecnologias.

De fato, as possibilidades são muitas, mas, se você está começando, vá com calma. Um passo de cada vez. Antes de se aventurar com visualizações interativas ou modelos estatísticos elaborados, é importante dominar o básico, como o uso de editores de planilha e a criação de gráficos simples.

De fato, muito se ouve falar em big data, mas saiba que [boas histórias também podem ser extraídas de planilhas](https://www.youtube.com/watch?v=2oFGpDFUnMc) de menor porte. A dica para começar a trabalhar com dados é encontrar algum tema que te motive por alguma razão, seja porque você tem paixão pelo assunto, seja porque é uma demanda do seu trabalho ou simplesmente algo que você gostaria aprender mais a fundo.

Não existe uma definição única para &#39;big data&#39;. Uma possível definição é que &#39;big data&#39; indica um volume de dados tão grande que um computador só não consegue processar. Você provavelmente não precisará disso em suas análises. Para bases muito grandes, você pode importar os dados em um banco de dados SQL, seja no seu próprio computador ou utilizando serviços online como o BigQuery da Google.

A ideia aqui é aprender a trabalhar com dados a partir de um &quot;mini-projeto&quot; real que te motive de alguma forma, buscando soluções para os problemas reais que enfrentar, ao invés de tentar dar conta de todas as possibilidades de aprendizado na área de antemão. Na próxima seção, falaremos sobre uma outra dúvida comum para quem está começando na área: é preciso aprender a programar para trabalhar com dados?

## Iniciação à programação

Ainda que não seja mais um obstáculo para começar a trabalhar com dados, saber programar de fato abre muitas possibilidades nessa área. Lidar com a obtenção, processamento, análise ou visualização de dados por meio de códigos te dá liberdade quase total para trabalhar. Você pode realizar as operações diretamente, sem as restrições de programas de terceiros, além de automatizar tarefas, documentar e compartilhar todas etapas de manipulação dos dados.

De modo geral, existem vantagens tanto nas soluções via interface gráfica, quanto nas baseadas em linguagens de programação.

**Vantagens de usar interfaces gráficas**

- Curva de aprendizado baixa: para leigos, é mais fácil aprender a usar programas com interfaces gráficas;
- Poupa tempo: especialmente se você não é fluente em alguma linguagem de programação, muitas vezes, poderá conseguir executar tarefas de forma mais rápida com interface gráfica.

**Vantagens de programar**

- Alta customização: o software será totalmente adaptável às suas necessidades;
- Automação: é muito fácil automatizar tarefas com programação, mas nem todos programas gráficos suportam isso;
- Escalabilidade: do mesmo modo, é fácil aumentar a escala de um software próprio para que ele lide com um volume maior de dados, por exemplo.

É comum, por exemplo, a criação de &quot;notebooks&quot; (blocos de notas), que trazem não só os dados, mas também os códigos que foram utilizados em determinado trabalho, além de comentários adicionais. Esta prática permite um excelente grau de transparência e detalhamento da conclusão e do método utilizado em trabalhos baseados em dados.

[img notebook]

É possível trabalhar com dados em qualquer linguagem de programação. Porém, iremos falar aqui especificamente sobre quatro linguagens que se destacam pela sua ampla utilização em projetos baseados em dados atualmente.

A primeira delas é o SQL, uma abreviação para linguagem de consulta estruturada. O SQL é um excelente ponto de partida para quem ter um primeiro contato com códigos para qualquer tipo de profissional, devido ao uso de palavras comuns em inglês para as operações (&quot;select&quot;,&quot;from&quot;, &quot;group by&quot;, etc). Esta linguagem incrivelmente simples permite consultar tabelas enormes, que &quot;travariam&quot; qualquer editor de planilha.

O SQL pode ser uma porta de entrada para o mundo da programação, visto que demanda o uso de lógica computacional. Para começar a usar SQL, você pode experimentar softwares como o DB Browser for SQLite e Dbeaver. Você pode também utilizar o SQL em conjunto com linguagens como Python e SQL, que falaremos a seguir, ao lidar com um volume grande de dados. Por ser uma tecnologia voltada especificamente para o armazenamento e processamento de bases de dados, o SQL tem uma performance superior às soluções nativas daquelas linguagens para lidar com muitos registros.

Outra opção é o JavaScript, uma linguagem especialmente interessante para designers ou pessoas interessadas em se aprofundar para valer com visualização de dados na web. Ao lado do HTML e do CSS, o JavaScript é uma das linguagens que dão as bases para as páginas web tal como a conhecemos. Mais especificamente, ela é responsável por permitir interatividade e dinamicidade entre os elementos de uma página, como quando você clica no botão de fechar de uma propaganda e ele some de sua tela.

O JavaScript não costuma ser tão utilizado em projetos de raspagem ou visualização de analisar dados, mas é especialmente interessante para visualização. A biblioteca D3.JS destaca-se como uma das principais referências para trabalhos do tipo.

Porém, se você está interessado em linguagens mais flexíveis, com as quais seja possível realizar com facilidade operações das diversas etapas do trabalho com dados, então, precisa conhecer as duas estrelas dessa área: Python e R.

Ambas são consideradas o &quot;estado da arte&quot; para projetos de ciência de dados e são amplamente utilizadas não só por cientistas, estatísticos ou desenvolvedores, mas também por jornalistas, pesquisadores das ciências humanas e organizações da sociedade civil.

Cada uma delas conta com milhares de bibliotecas ou pacotes, que nada mais são que softwares escritos nas respectivas linguagens que expandem suas funcionalidades nativas para melhor performar tarefas específicas. A quantidade de pacotes e bibliotecas disponíveis é surpreendente. Suas funcionalidades são muitas: podem servir para estruturar dados a partir de páginas na web, criar visualizações de dados, modelos estatísticos, rotinas de automação e etc.

Para quem está começando, os comandos básicos de ambas são igualmente simples, mas a real complexidade de cada linguagem vai depender do seu objetivo, dos softwares já disponíveis para facilitar esta tarefa e do tempo/conhecimento disponível por você ou sua equipe.

Você pode escolher uma ou aprender ambas, o que te dará bastante flexibilidade. Mas vamos aqui destacar algumas características gerais sobre estas linguagens para você se situar.

De acordo com a [revista do](https://spectrum.ieee.org/static/interactive-the-top-programming-languages-2020)[Institute of Electrical and Electronics Engineers](https://spectrum.ieee.org/static/interactive-the-top-programming-languages-2020), Python foi considerada a linguagem mais popular do mundo. Flexível e moderna, permite a utilização em aplicações diversas ao mesmo tempo em que mantém uma curva de aprendizado suave, com códigos simples e legíveis. Python é uma linguagem robusta, que pode ser utilizada não só para trabalhar com dados, como também para distintas aplicações (Web, administração de redes entre outras). O índice de pacotes em Python conta quase 250 mil projetos disponíveis.

Já o R costuma ser preferido por estatísticos e acadêmicos de diversas áreas. A linguagem leva a fama de ter bibliotecas que produzem gráficos e visualizações mais caprichadas e também conta com um rico universo de bibliotecas de softwares para trabalhar com dados. Atualmente, no [CRAN](https://cran.r-project.org/web/packages/available_packages_by_name.html), o principal repositório da linguagem, estão listadas mais de 16 mil bibliotecas - isso sem contar outras bibliotecas disponíveis em outra fontes.

###

